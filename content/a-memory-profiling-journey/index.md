+++
title = "A memory profiling journey"
description="I walk through the process of discovering the consequences of not having backpressure in a system"
date = 2025-06-21

[taxonomies]
categories = ["programming", "rust", "profiling"]

[extra]
+++

- What are we trying to do here?
  - Increased memory usage in C++ wrapper of our Rust library.
  - Wrapper is structured with main entry point which spawns background thread per producer instance
  - `SendMessage(producer_id, message)`
- Profiling anything requires a baseline.
  - How do you set up the baseline.
  - What tools do we have access to?
    - [`stats_alloc`](https://docs.rs/stats_alloc/latest/stats_alloc/)
    - [`dhat-rs`](https://docs.rs/dhat/latest/dhat/)
  - Setting up a baseline test
    - Different feature flags for different kind of profiling.
    - How to understand the output
      - Especially for the output of dhat-rs.
- Initial changes
  - Identified a possibly unnecessary clone
  - Turned out it did not make a difference.
- dhat viewer pointed out a majority of the allocations were alive for 50% of the runtime of the program.
- Looked into the lifetime of an allocations 
  - `Vec<u8>` created by copying contents of the payload when sending the message
  - Message sent to a background thread spawned per producer.
  - Background thread moves the message into the actual `Produce` call.
  - The thread function gets a handle to the runtime and `block_on`'s the `produce` call.
  - The produce call internally sends the message to an actor which buffers and encodes the request.
- Added `created_time` to the main `Message` struct.
  - Added print statements tracking timing at every step.
- Messages sitting in FFI layer queue immediately.
- Two things immediately stood out:
  - The FFI queue to process messages did not have a bound.
  - Using `block_on` made the processing of `Message`s serial, only one messages sent in a single batch.
- How do we fix this:
  - Adding a configurable bound to the FFI layer queue.
    - Use `recv_many` in the background process to ensure batches are sent.
    - This adds backpressure to the system as the thread calling `sendMessage` will block until there is space on the channel to send a message.
    - Consequently reduces memory usage as most of the allocations are hanging around waiting to be processed.


# Prelude

Imagine if you will, that you want to create a library for sending a message to an external service. This service provides an API client library which you plan on using. This library has an API that which looks a little something like this:

```rust

pub struct Producer {
  //
}

impl Producer {
  pub fn new() -> Self {}

  pub async fn produce(message: Message) -> Result<(), Error> {}

  pub async fn produce_batch(message: Message) -> Result<(), Error> {}
}
```

And a contrived example of using this API might look like this:

```rust

async fn produce_message(key: String, payload: Vec<u8>) -> Result<(), String> {
  let producer = Producer::new("http://example.endpoint");

  if let OK(_) = producer.produce(key, payload).await {
    return Ok(());
  } else {
    return Err("Error sending message to API".into())
  }
}
```

The specifics of the underlying protocol don't matter. The main behavior that the API client exposes is two `async` methods that make the requests, and wait for a response before returning control back to the caller. While the method won't block the entire thread when it is waiting for a response, the task that is executing the future generated by this method will be suspended until the underlying IO object notifies the runtime that a response came through. Therefore, functionally, when called from another `async` context, the task is blocking.

Additionally, say you want to provide a for your users.
{% sidenote(note_name="sync-reasons", inline_segment="synchronous API " ) %}
    This could be for a variety of reasons, maybe your users are calling your library from another language over an FFI layer. Maybe you just don't want to deal with the <a href="https://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/">Function coloring problem</a>
{% end %}
To address this, you reach for a fairly standard solution -- spawn a background task on a runtime that you initialize:

```rust
pub struct Telegrapher {
    tx: mpsc::Sender<Message>,
}

impl Telegrapher {
    pub fn new(endpoint: String) -> Self {
        let producer = Producer::new(endpoint);

        let (tx, rx) = mpsc::channel::<Message>();

        thread::spawn(|| {
            while let Some(message) = rx.recv() {
                // Gets a handle to ~~Some~~ runtime
                get_runtime().block_on(async {
                    producer
                        .produce(message)
                        .await
                        .expect("We're ignoring error handling for now");
                });
            }
        });

        Self { tx }
    }

  pub fn send_message(message: Message) {
    self.tx.send(message)
  }
}

```

Nice! You now have a simple library you can ship off to customers, you can even use [bindgen][bindgen] to generate a C library which can be used by consumers from other language ecosystems. Happy, you mark your task closed and move on with your life. But reader, did you catch the mistakes lurking in the previous snippet?

Soon, a customer reaches out to you, calling out that your library is using more memory than their resource limits allow. They are running on a resource constrained embedded system, and your library is using 25 MB of the 50 MB memory budget that they have available, and therefore they are unable to service user requests. And thus starts a very fun investigation that I found myself performing over the last week.

[bindgen]: https://github.com/rust-lang/rust-bindgen

# Establishing a baseline

In order to even understand whether or not I could make improvements and identify which memory allocations are causing the problem, I first need to establish a baseline. If this were linux or macos, there are various tools available to do this:

- [Valgrind]: 
- [DHAT]
- [Heaptrack]

But I was running on Windows, where the options for memory profilers were rather limited.
